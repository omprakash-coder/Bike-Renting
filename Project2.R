#Clear the working Environment
rm(list=ls())

#Set the working path
setwd("D:/Data Science Edwisor/Project_2")

#Get the working Directory
getwd()


#combining all the libraries and storing in x
x = c("ggplot2", "corrgram", "DMwR", "caret", "randomForest", "unbalanced", "C50", "dummies", "e1071", "Information",
      "MASS", "rpart", "gbm", "ROSE", 'sampling', 'DataCombine', 'inTrees','fastDummies')

#lapply function is applied for operations on list objects and returns a list object
lapply(x, require, character.only = TRUE)#loading multiple packages at once

#storing day.csv data in bike
bike = read.csv("day.csv", header = T, na.strings = c(" ", "", "NA"))

#storing bike data in bike_train
bike_train=bike

#Exploratory Data Analysis
#$ sign is used to extract the particular column.
#The function factor is used to encode a vector as a factor
bike_train$season=as.factor(bike_train$season)
bike_train$mnth=as.factor(bike_train$mnth)
bike_train$yr=as.factor(bike_train$yr)
bike_train$holiday=as.factor(bike_train$holiday)
bike_train$weekday=as.factor(bike_train$weekday)
bike_train$workingday=as.factor(bike_train$workingday)
bike_train$weathersit=as.factor(bike_train$weathersit)

#selecting elements of data
bike_train=subset(bike_train,select = -c(instant,casual,registered))

d1=unique(bike_train$dteday)#unique returns a vector, data frame with duplicate elements
df=data.frame(d1)
bike_train$dteday=as.Date(df$d1,format="%Y-%m-%d")
df$d1=as.Date(df$d1,format="%Y-%m-%d")
bike_train$dteday=format(as.Date(df$d1,format="%Y-%m-%d"), "%d")
bike_train$dteday=as.factor(bike_train$dteday)

#Str is best for displaying contents of lists
str(bike_train)

#Missing Values Analysis
#checking for missing value
missing_val = data.frame(apply(bike_train,2,function(x){sum(is.na(x))}))

#Outlier Analysis
#BoxPlots - Distribution and Outlier Check
numeric_index = sapply(bike_train,is.numeric) #selecting only numeric

numeric_data = bike_train[,numeric_index]

cnames = colnames(numeric_data)

for (i in 1:length(cnames))
{
  assign(paste0("gn",i), ggplot(aes_string(y = (cnames[i]), x = "cnt"), data = subset(bike_train))+ 
           stat_boxplot(geom = "errorbar", width = 0.5) +
           geom_boxplot(outlier.colour="red", fill = "blue" ,outlier.shape=18,
                        outlier.size=1, notch=FALSE) +
           theme(legend.position="bottom")+
           labs(y=cnames[i],x="cnt")+
           ggtitle(paste("Box plot of count for",cnames[i])))
}

#arrange multiple ggplots on one page.
gridExtra::grid.arrange(gn1,gn2,ncol=3)
gridExtra::grid.arrange(gn3,gn4,ncol=2)



#Feature Selection
#Correlation Plot 
corrgram(bike_train[,numeric_index], order = F,
         upper.panel=panel.pie, text.panel=panel.txt, main = "Correlation Plot")



#Dimension Reduction
bike_train = subset(bike_train,select = -c(atemp))


#Model Development
train_index = sample(1:nrow(bike_train), 0.8 * nrow(bike_train))
train = bike_train[train_index,]
test = bike_train[-train_index,]

#Decision tree regression
fit = rpart(cnt ~ ., data = train, method = "anova")
predictions_DT = predict(fit, test[,-12])

#Random Forest Model
install.packages("randomForest")
library(randomForest)

RF_model = randomForest(cnt ~ ., train, importance = TRUE, ntree = 200)
predictions_RF = predict(RF_model, test[,-12])
plot(RF_model)

#Linear Regression
#converting multilevel categorical variable into binary dummy variable
cnames= c("dteday","season","mnth","weekday","weathersit")
data_lr=bike_train[,cnames]
cnt=data.frame(bike_train$cnt)
names(cnt)[1]="cnt"

#installing fastDummies packages
install.packages("fastDummies")
library(fastDummies)#provides a significant speed increase from creating dummy variables through model.
data_lr <- fastDummies::dummy_cols(data_lr)
data_lr= subset(data_lr,select = -c(dteday,season,mnth,weekday,weathersit))
d3 = cbind(data_lr,bike_train)
d3= subset(d3,select = -c(dteday,season,mnth,weekday,weathersit,cnt))
data_lr=cbind(d3,cnt)


#dividind data into test and train
train_index = sample(1:nrow(data_lr), 0.8 * nrow(data_lr))
train_lr = data_lr[train_index,]
test_lr = data_lr[-train_index,]

#Linear regression model making
lm_model = lm(cnt ~., data = train_lr)
predictions_LR = predict(lm_model,test_lr[,-64])


#evaluating MApe value
#is a measure of prediction accuracy
MAPE = function(y, yhat){
  mean(abs((y - yhat)/y))*100
}
MAPE(test[,12], predictions_DT)

MAPE(test[,12], predictions_RF)

MAPE(test_lr[,64],  predictions_LR)



#extacting predicted values output from Random forest model
results <- data.frame(test, pred_cnt = predictions_RF)

write.csv(results, file = 'RF output R .csv', row.names = FALSE, quote=FALSE)
